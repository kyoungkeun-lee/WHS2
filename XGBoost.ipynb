{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10g3ddhqC7s_SzibGyTjHr9GzZxznB_dt",
      "authorship_tag": "ABX9TyPPVsT/bz3f6PDcKix0zrps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyoungkeun-lee/WHS2/blob/main/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0O8a028n2Ok",
        "outputId": "4b164dd9-2b96-4239-c283-be246f13be23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "Error parsing line: \n",
            "\n",
            "Error: Expecting value: line 2 column 1 (char 1)\n",
            "  timestamp  formatVersion                                           webaclId  \\\n",
            "0     02:31              1  arn:aws:wafv2:ap-northeast-2:175518383537:regi...   \n",
            "1     02:31              1  arn:aws:wafv2:ap-northeast-2:175518383537:regi...   \n",
            "2     02:31              1  arn:aws:wafv2:ap-northeast-2:175518383537:regi...   \n",
            "3     02:31              1  arn:aws:wafv2:ap-northeast-2:175518383537:regi...   \n",
            "4     02:31              1  arn:aws:wafv2:ap-northeast-2:175518383537:regi...   \n",
            "\n",
            "               terminatingRuleId terminatingRuleType action  \\\n",
            "0                 Default_Action             REGULAR  ALLOW   \n",
            "1                 Default_Action             REGULAR  ALLOW   \n",
            "2  WAFSecurityAutomationsXssRule             REGULAR  BLOCK   \n",
            "3  WAFSecurityAutomationsXssRule             REGULAR  BLOCK   \n",
            "4                 Default_Action             REGULAR  ALLOW   \n",
            "\n",
            "                         terminatingRuleMatchDetails httpSourceName  \\\n",
            "0                                                 []            ALB   \n",
            "1                                                 []            ALB   \n",
            "2  [{'conditionType': 'XSS', 'location': 'QUERY_S...            ALB   \n",
            "3  [{'conditionType': 'XSS', 'location': 'QUERY_S...            ALB   \n",
            "4                                                 []            ALB   \n",
            "\n",
            "                                        httpSourceId ruleGroupList  ...  \\\n",
            "0  175518383537-app/MYDVWA-Appli-joedwOCI5mYH/55f...            []  ...   \n",
            "1  175518383537-app/MYDVWA-Appli-joedwOCI5mYH/55f...            []  ...   \n",
            "2  175518383537-app/MYDVWA-Appli-joedwOCI5mYH/55f...            []  ...   \n",
            "3  175518383537-app/MYDVWA-Appli-joedwOCI5mYH/55f...            []  ...   \n",
            "4  175518383537-app/MYDVWA-Appli-joedwOCI5mYH/55f...            []  ...   \n",
            "\n",
            "  httpRequest.clientIp httpRequest.country  \\\n",
            "0       218.39.203.102                  KR   \n",
            "1       218.39.203.102                  KR   \n",
            "2       218.39.203.102                  KR   \n",
            "3       218.39.203.102                  KR   \n",
            "4       218.39.203.102                  KR   \n",
            "\n",
            "                                 httpRequest.headers httpRequest.uri  \\\n",
            "0  [{'name': 'Host', 'value': 'mydvwa-appli-joedw...        REDACTED   \n",
            "1  [{'name': 'Host', 'value': 'mydvwa-appli-joedw...        REDACTED   \n",
            "2  [{'name': 'Host', 'value': 'mydvwa-appli-joedw...        REDACTED   \n",
            "3  [{'name': 'Host', 'value': 'mydvwa-appli-joedw...        REDACTED   \n",
            "4  [{'name': 'Host', 'value': 'mydvwa-appli-joedw...        REDACTED   \n",
            "\n",
            "  httpRequest.args httpRequest.httpVersion httpRequest.httpMethod  \\\n",
            "0         REDACTED                HTTP/1.1               REDACTED   \n",
            "1         REDACTED                HTTP/1.1               REDACTED   \n",
            "2         REDACTED                HTTP/1.1               REDACTED   \n",
            "3         REDACTED                HTTP/1.1               REDACTED   \n",
            "4         REDACTED                HTTP/1.1               REDACTED   \n",
            "\n",
            "                 httpRequest.requestId requestBodySize  \\\n",
            "0  1-6674e5fc-6350c85d09a59e9c7966b133             NaN   \n",
            "1  1-6674e5fd-2ee6cf4046f13780276960d0             NaN   \n",
            "2  1-6674e605-6571bdda2df2a43867fa57d1             NaN   \n",
            "3  1-6674e603-4ec519dc54fb1fc4544184fb             NaN   \n",
            "4  1-6674e605-75807462325f09a768972d95             NaN   \n",
            "\n",
            "  requestBodySizeInspectedByWAF  \n",
            "0                           NaN  \n",
            "1                           NaN  \n",
            "2                           NaN  \n",
            "3                           NaN  \n",
            "4                           NaN  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# 파일 경로 설정\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/dataset/nsl-kdd/data.txt'\n",
        "\n",
        "# 파일 읽기\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# 각 행을 개별적으로 파싱\n",
        "parsed_data = []\n",
        "for line in lines:\n",
        "    try:\n",
        "        parsed_data.append(json.loads(line))\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing line: {line}\")\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# JSON 데이터를 데이터프레임으로 변환\n",
        "df = pd.json_normalize(parsed_data)\n",
        "\n",
        "# timestamp를 변환하는 함수 정의\n",
        "def convert_epoch_to_time(epoch):\n",
        "    # Epoch 시간을 datetime 객체로 변환\n",
        "    dt = datetime.fromtimestamp(epoch / 1000.0)\n",
        "    # 시간을 '시:분' 형식으로 변환하여 반환\n",
        "    return dt.strftime('%H:%M')\n",
        "\n",
        "# timestamp 칼럼 변환\n",
        "df['timestamp'] = df['timestamp'].apply(convert_epoch_to_time)\n",
        "\n",
        "# 정리된 데이터프레임 확인\n",
        "print(df.head())\n",
        "\n",
        "# 정리된 데이터프레임을 CSV 파일로 저장\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/dataset/nsl-kdd/TestFinal.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/dataset/nsl-kdd/TestFinal.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop columns that are not useful for modeling\n",
        "columns_to_drop = ['webaclId', 'terminatingRuleId', 'terminatingRuleType',\n",
        "                   'terminatingRuleMatchDetails', 'httpSourceName', 'httpSourceId',\n",
        "                   'ruleGroupList', 'rateBasedRuleList', 'nonTerminatingMatchingRules',\n",
        "                   'excludedRules', 'httpRequest.headers', 'httpRequest.uri',\n",
        "                   'httpRequest.args', 'httpRequest.httpVersion', 'httpRequest.requestId']\n",
        "\n",
        "actual_columns_to_drop = [col for col in columns_to_drop if col in data.columns]\n",
        "data = data.drop(columns=actual_columns_to_drop)\n",
        "\n",
        "# Drop columns with persistent missing values\n",
        "columns_with_missing_values = ['requestHeadersInserted', 'responseCodeSent']\n",
        "data = data.drop(columns=columns_with_missing_values)\n",
        "\n",
        "# Encode categorical variables before handling missing values\n",
        "label_encoders = {}\n",
        "categorical_columns = ['action', 'httpRequest.clientIp', 'httpRequest.country', 'httpRequest.httpMethod','timestamp']\n",
        "\n",
        "for column in categorical_columns:\n",
        "    if column in data.columns:\n",
        "        le = LabelEncoder()\n",
        "        data[column] = le.fit_transform(data[column])\n",
        "        label_encoders[column] = le\n",
        "\n",
        "# Handle missing values by filling with the median for numeric columns and the most frequent value for categorical columns\n",
        "for column in data.columns:\n",
        "    if data[column].dtype in [int, float]:\n",
        "        data[column].fillna(data[column].median(), inplace=True)\n",
        "    else:\n",
        "        data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "# Check for any remaining missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing values per column after imputation:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Verify there are no more NaN values\n",
        "assert data.isnull().sum().sum() == 0, \"There are still missing values in the dataset\"\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=['action'])\n",
        "y = data['action']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Save the preprocessed data for further steps\n",
        "X_scaled_filename = 'X_scaled.joblib'\n",
        "y_filename = 'y.joblib'\n",
        "joblib.dump(X_scaled, X_scaled_filename)\n",
        "joblib.dump(y, y_filename)\n",
        "\n",
        "# Confirm the files have been saved\n",
        "X_scaled_filename, y_filename\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the preprocessed data\n",
        "X_scaled_filename = 'X_scaled.joblib'\n",
        "y_filename = 'y.joblib'\n",
        "X_scaled = joblib.load(X_scaled_filename)\n",
        "y = joblib.load(y_filename)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the parameters for the XGBoost model to speed up training\n",
        "params = {\n",
        "    'n_estimators': 50,            # Further reduce the number of boosting rounds\n",
        "    'max_depth': 3,                # Reduce the maximum depth of trees\n",
        "    'learning_rate': 0.1,          # Set the learning rate\n",
        "    'eval_metric': 'logloss',      # Evaluation metric\n",
        "    'use_label_encoder': False,    # Disable label encoder usage\n",
        "    'early_stopping_rounds': 10,   # Early stopping\n",
        "}\n",
        "\n",
        "# Use a smaller subset of the data for quick evaluation\n",
        "X_train_subset, _, y_train_subset, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
        "\n",
        "# Train the XGBoost model with the specified parameters on the subset\n",
        "model = xgb.XGBClassifier(**params)\n",
        "model.fit(X_train_subset, y_train_subset, eval_set=[(X_test, y_test)], verbose=False)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "# Save the trained model\n",
        "model_filename = '/content/drive/MyDrive/Colab Notebooks/dataset/nsl-kdd/xgboost_model.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "\n",
        "f1, model_filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdFzEmtdDXQl",
        "outputId": "0c28cae0-56c6-41ca-9da1-2f510eec5a3d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column after imputation:\n",
            "Series([], dtype: int64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9262394195888755,\n",
              " '/content/drive/MyDrive/Colab Notebooks/dataset/nsl-kdd/xgboost_model.joblib')"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ]
}